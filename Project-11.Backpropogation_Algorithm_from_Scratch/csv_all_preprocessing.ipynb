{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Break csv rows and generate multiple csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "dir = r\"./csv_A\"\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "    \n",
    "with open('./A.csv') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader) #skip header\n",
    "    \n",
    "    #Group by column (id)\n",
    "    lst = sorted(reader, key=lambda x : x[0])\n",
    "    groups = groupby(lst, key=lambda x : x[0])\n",
    "\n",
    "    #Write file for each id\n",
    "    for k,g in groups:\n",
    "        filename = k + '.csv'\n",
    "        with open(os.path.join(dir,filename), 'w', newline='') as fout:\n",
    "            csv_output = csv.writer(fout)\n",
    "            csv_output.writerow([\"id\",\"type\",\"x\",\"y\",\"z\", \"vz\", \"TIMESTEP\"])  #header\n",
    "            #sortedlist = sorted(lst, key=lambda row: row[5], reverse=True)\n",
    "            for line in g:\n",
    "                #csv_output.writerow(line)\n",
    "                csv_output.writerow(line)\n",
    "                #df.to_csv(r'./csv/File Name.csv', index = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "dir = r\"./csv_B\"\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "    \n",
    "with open('./B.csv') as csv_file:\n",
    "    reader = csv.reader(csv_file)\n",
    "    next(reader) #skip header\n",
    "    \n",
    "    #Group by column (id)\n",
    "    lst = sorted(reader, key=lambda x : x[0])\n",
    "    groups = groupby(lst, key=lambda x : x[0])\n",
    "\n",
    "    #Write file for each id\n",
    "    for k,g in groups:\n",
    "        filename = k + '.csv'\n",
    "        with open(os.path.join(dir,filename), 'w', newline='') as fout:\n",
    "            csv_output = csv.writer(fout)\n",
    "            csv_output.writerow([\"id\",\"type\",\"x\",\"y\",\"z\", \"vz\", \"TIMESTEP\"])  #header\n",
    "            #sortedlist = sorted(lst, key=lambda row: row[5], reverse=True)\n",
    "            for line in g:\n",
    "                #csv_output.writerow(line)\n",
    "                csv_output.writerow(line)\n",
    "                #df.to_csv(r'./csv/File Name.csv', index = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort csv columns at once in multiple csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import operator\n",
    "import glob\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = dict() # filename : lists\n",
    "path = \"./csv_A/*.csv\"\n",
    "\n",
    "files=glob.glob(path)\n",
    "for filename in files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df.sort_values('TIMESTEP', inplace=True)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict() # filename : lists\n",
    "\n",
    "path = \"./csv_B/*.csv\"\n",
    "\n",
    "files=glob.glob(path)\n",
    "for filename in files:\n",
    "    df = pd.read_csv(filename)\n",
    "    df.sort_values('TIMESTEP', inplace=True)\n",
    "    df.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined all csv files in one csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from natsort import natsorted\n",
    "\n",
    "dir = r\"./original_data\"\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in natsorted(glob.glob('./csv_A/*.{}'.format(extension)))]\n",
    "#srt_files = natsorted(all_filenames)\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv.to_csv( os.path.join(dir , \"1.csv\"), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from natsort import natsorted\n",
    "\n",
    "dir = r\"./original_data\"\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in natsorted(glob.glob('./csv_B/*.{}'.format(extension)))]\n",
    "#srt_files = natsorted(all_filenames)\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv.to_csv( os.path.join(dir , \"2.csv\"), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from natsort import natsorted\n",
    "\n",
    "dir = r\"./original_data/\"\n",
    "#if not os.path.exists(dir):\n",
    "#    os.mkdir(dir)\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in natsorted(glob.glob('./original_data/*.{}'.format(extension)))]\n",
    "#srt_files = natsorted(all_filenames)\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv.to_csv( os.path.join(dir , \"AB.csv\"), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select only two columns from csv and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dir = r\"./original_data/\"\n",
    "df = pd.read_csv('./original_data/1.csv' , usecols = ['z','id'])\n",
    "df.to_csv(os.path.join(dir,\"sort_A.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dir = r\"./original_data/\"\n",
    "df = pd.read_csv('./original_data/2.csv' , usecols = ['z','id'])\n",
    "df.to_csv(os.path.join(dir,\"sort_B.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transpose columns to rows (csv only with 2 columns) and make a new csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "dir = r\"./original_data/\"\n",
    "\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "    \n",
    "#root = \"./original_data/\"\n",
    "with open(dir + 'sort_A.csv') as f_csv:\n",
    "    csv_reader = csv.DictReader(f_csv)\n",
    "\n",
    "    result = {}\n",
    "    req_dict = defaultdict(list)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        for column, value in row.items():\n",
    "            result.setdefault(column, []).append(value)\n",
    "        req_dict[row['id'].upper()].append(row['z'])\n",
    "\n",
    "with open(os.path.join(dir,'only_position_A.csv'), 'w') as f:\n",
    "    c = csv.writer(f)\n",
    "\n",
    "    for key, value in req_dict.items():\n",
    "        c.writerow([key] + value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "#dir = r\"./data/\"\n",
    "dir = \"./original_data/\"\n",
    "if not os.path.exists(dir):\n",
    "    os.mkdir(dir)\n",
    "\n",
    "with open(dir + 'sort_B.csv') as f_csv:\n",
    "    csv_reader = csv.DictReader(f_csv)\n",
    "\n",
    "    result = {}\n",
    "    req_dict = defaultdict(list)\n",
    "\n",
    "    for row in csv_reader:\n",
    "        for column, value in row.items():\n",
    "            result.setdefault(column, []).append(value)\n",
    "        req_dict[row['id'].upper()].append(row['z'])\n",
    "\n",
    "with open(os.path.join(dir,'only_position_B.csv'), 'w') as f:\n",
    "    c = csv.writer(f)\n",
    "\n",
    "    for key, value in req_dict.items():\n",
    "        c.writerow([key] + value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_add = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./original_data/only_position_A.csv\", 'r') as readFile:\n",
    "    rd = csv.reader(readFile)\n",
    "    lines = list(rd)\n",
    "    lines.insert(0, header_add)\n",
    "\n",
    "with open(\"./original_data/with_header_A.csv\", 'w',newline='') as writeFile:\n",
    "    wt = csv.writer(writeFile)\n",
    "    wt.writerows(lines)\n",
    "\n",
    "readFile.close()\n",
    "writeFile.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./original_data/only_position_B.csv\", 'r') as readFile:\n",
    "    rd = csv.reader(readFile)\n",
    "    lines = list(rd)\n",
    "    lines.insert(0, header_add)\n",
    "\n",
    "with open(\"./original_data/with_header_B.csv\", 'w',newline='') as writeFile:\n",
    "    wt = csv.writer(writeFile)\n",
    "    wt.writerows(lines)\n",
    "\n",
    "readFile.close()\n",
    "writeFile.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add extra column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./data/\"\n",
    "\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "#old_file = pd.read_csv('./data/only_position_A.csv', header=None)\n",
    "\n",
    "#old_file.to_csv(os.path.join(dir,\"1_A.csv\"),headers = ['id',\"z-1\",\"z-2\",\"z-3\",\"z-4\",\"z-5\",\"z-6\",\"z-7\",\"z-8\",\"z-9\",\"z-10\",\"z-11\" ,\"z-12\",\"z-13\",\"z-14\",\"z-15\",\"z-16\",\"z-17\",\"z-18\",\"z-19\",\"z-20\",\"z-21\",\"z-22\",\"z-23\",\"z-24\",\"z-25\",\"z-26\",\"z-27\",\"z-28\",\"z-29\",\"z-30\",\"z-31\",\"z-32\",\"z-33\",\"z-34\"], index=False) \n",
    "import pandas as pd  \n",
    "aa = pd.read_csv(\"./original_data/with_header_A.csv\")  \n",
    "aa.insert(0, column = \"type\", value = \"0\")  \n",
    "aa.head()\n",
    "aa.to_csv(os.path.join(root,\"1.csv\"), index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"./data/\"\n",
    "\n",
    "if not os.path.exists(root):\n",
    "    os.mkdir(root)\n",
    "\n",
    "#old_file = pd.read_csv('./data/only_position_A.csv', header=None)\n",
    "\n",
    "#old_file.to_csv(os.path.join(dir,\"1_A.csv\"),headers = ['id',\"z-1\",\"z-2\",\"z-3\",\"z-4\",\"z-5\",\"z-6\",\"z-7\",\"z-8\",\"z-9\",\"z-10\",\"z-11\" ,\"z-12\",\"z-13\",\"z-14\",\"z-15\",\"z-16\",\"z-17\",\"z-18\",\"z-19\",\"z-20\",\"z-21\",\"z-22\",\"z-23\",\"z-24\",\"z-25\",\"z-26\",\"z-27\",\"z-28\",\"z-29\",\"z-30\",\"z-31\",\"z-32\",\"z-33\",\"z-34\"], index=False) \n",
    "import pandas as pd  \n",
    "aa = pd.read_csv(\"./original_data/with_header_B.csv\")  \n",
    "aa.insert(0, column = \"type\", value = \"1\")  \n",
    "aa.head()\n",
    "aa.to_csv(os.path.join(root,\"2.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sort out new csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import shutil\n",
    "from natsort import natsorted\n",
    "\n",
    "dir = r\"./data/\"\n",
    "#if not os.path.exists(dir):\n",
    "#    os.mkdir(dir)\n",
    "extension = 'csv'\n",
    "all_filenames = [i for i in natsorted(glob.glob('./data/*.{}'.format(extension)))]\n",
    "#srt_files = natsorted(all_filenames)\n",
    "combined_csv = pd.concat([pd.read_csv(f) for f in all_filenames ])\n",
    "combined_csv.to_csv( os.path.join(dir , \"merge.csv\"), index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sort a single column in single csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import operator\n",
    "import glob\n",
    "import pandas as pd\n",
    "\n",
    "data = dict() # filename : lists\n",
    "\n",
    "path=\"./data/merge.csv\"\n",
    "files=glob.glob(path)\n",
    "for filename in files:\n",
    "  df = pd.read_csv(filename)\n",
    "  df.sort_values('id', inplace=True)\n",
    "  df.to_csv(filename, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "in_file = open(\"./data/merge.csv\", \"r\")\n",
    "out_file = open(\"./data/clean.csv\", \"w\", newline=\"\")\n",
    "\n",
    "in_csv = csv.reader(in_file)\n",
    "out_csv = csv.writer(out_file)\n",
    "\n",
    "for row_number, row in enumerate(in_csv):\n",
    "    if row_number <= 212:\n",
    "        out_csv.writerow(row[:60])\n",
    "\n",
    "in_file.close()\n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# re arrange csv for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "header_add_again = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./data/clean.csv')\n",
    "df_reorder = df[header_add_again] # rearrange column here\n",
    "df_reorder.to_csv('./data/train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
